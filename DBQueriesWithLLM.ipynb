{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  We first add the Database Schema, so the LLM knows how to generate the queries.\n",
    "\n",
    "We do this by using the datatables CREATE statements, and set them to the DB_SCHEMA Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_SCHEMA = \"\"\"\n",
    "CREATE TABLE WorkflowReporting.WorkflowInstances (\n",
    "    WorkflowID INT IDENTITY(1,1) PRIMARY KEY,  -- Unique identifier for each workflow instance\n",
    "    ProcessInstanceID UNIQUEIDENTIFIER NOT NULL,  -- Unique identifier for the process instance\n",
    "    Status NVARCHAR(50) NOT NULL CHECK (Status IN ('Started', 'In Progress', 'Completed', 'Failed', 'Cancelled')),  -- Status of the workflow\n",
    "    LinkToProcess NVARCHAR(500) NULL,  -- URL or reference link to the process\n",
    "    Departments NVARCHAR(255) NULL,  -- Departments involved\n",
    "    StartDate DATETIME2 NOT NULL DEFAULT GETUTCDATE(),  -- Start timestamp\n",
    "    EndDate DATETIME2 NULL,  -- End timestamp\n",
    "    BusinessData NVARCHAR(MAX) NULL,  -- Flexible JSON column for additional business data\n",
    "    CreatedAt DATETIME2 NOT NULL DEFAULT GETUTCDATE(),  -- Record creation timestamp\n",
    "    UpdatedAt DATETIME2 NULL DEFAULT GETUTCDATE(),  -- Last update timestamp\n",
    "    CONSTRAINT CK_Status CHECK (Status IN ('Started', 'In Progress', 'Completed', 'Failed', 'Cancelled'))\n",
    ");\n",
    "\n",
    "CREATE TABLE WorkflowReporting.WorkflowUserTasks (\n",
    "    TaskID INT IDENTITY(1,1) PRIMARY KEY,  -- Unique identifier for each task\n",
    "    WorkflowID INT NOT NULL,  -- Links to WorkflowInstances table\n",
    "    TaskName NVARCHAR(255) NOT NULL,  -- Name of the task\n",
    "    AssignedTo NVARCHAR(255) NULL,  -- User assigned to the task\n",
    "    Status NVARCHAR(50) NOT NULL CHECK (Status IN ('Pending', 'In Progress', 'Completed', 'Failed', 'Cancelled')),  -- Task status\n",
    "    Priority NVARCHAR(20) NOT NULL DEFAULT 'Normal' CHECK (Priority IN ('Low', 'Normal', 'High', 'Critical')), -- Priority of task\n",
    "    DueDate DATETIME2 NULL,  -- Task due date\n",
    "    CompletedDate DATETIME2 NULL,  -- Completion timestamp\n",
    "    BusinessData NVARCHAR(MAX) NULL,  -- JSON column for additional task-specific business data\n",
    "    CreatedAt DATETIME2 NOT NULL DEFAULT GETUTCDATE(),  -- Record creation timestamp\n",
    "    UpdatedAt DATETIME2 NULL DEFAULT GETUTCDATE(),  -- Last update timestamp\n",
    "    CONSTRAINT FK_Workflow_UserTask FOREIGN KEY (WorkflowID) REFERENCES WorkflowReporting.WorkflowInstances(WorkflowID) ON DELETE CASCADE\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add some sample quries to help the LLM out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_EXAMPLES = [\n",
    "    {\n",
    "        'request': 'show me records where foobar is false',\n",
    "        'response': \"SELECT * FROM records WHERE JSON_VALUE(attributes, '$.foobar') = 'false'\",\n",
    "    },\n",
    "    {\n",
    "        'request': 'show me records where attributes include the key \\\"foobar\\\"',\n",
    "        'response': \"SELECT * FROM records WHERE JSON_QUERY(attributes, '$.foobar') IS NOT NULL\",\n",
    "    },\n",
    "    {\n",
    "        'request': 'show me records from yesterday',\n",
    "        'response': \"SELECT * FROM records WHERE CAST(start_timestamp AS DATE) = CAST(DATEADD(DAY, -1, GETDATE()) AS DATE)\",\n",
    "    },\n",
    "    {\n",
    "        'request': 'show me error records with the tag \\\"foobar\\\"',\n",
    "        'response': \"SELECT * FROM records WHERE level = 'error' AND 'foobar' IN (SELECT value FROM STRING_SPLIT(tags, ','))\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use pydantic to help create some Models or classes for defining our responses\n",
    "\n",
    "This means with also have to load those modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Any, Union\n",
    "from annotated_types import MinLen\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Success(BaseModel):\n",
    "    \"\"\"Response when SQL could be successfully generated.\"\"\"\n",
    "\n",
    "    sql_query: Annotated[str, MinLen(1)]\n",
    "    explanation: str = Field(\n",
    "        '', description='Explanation of the SQL query, as markdown'\n",
    "    )\n",
    "\n",
    "\n",
    "class InvalidRequest(BaseModel):\n",
    "    \"\"\"Response the user input didn't include enough information to generate SQL.\"\"\"\n",
    "\n",
    "    error_message: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define our Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Any, Union\n",
    "from typing_extensions import TypeAlias\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "Response: TypeAlias = Union[Success, InvalidRequest]\n",
    "agent: Agent[ Response] = Agent(\n",
    "    'openai:gpt-4o',\n",
    "    # Type ignore while we wait for PEP-0747, nonetheless unions will work fine everywhere else\n",
    "    result_type=Response,  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define our system prompt.  This is just extra text that goes to the LLM along with the users questions\n",
    "\n",
    "For this example, we are sending in the DB schema we created before, so the LLM will know how to build the query.\n",
    "\n",
    "We also need to import a few helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from pydantic_ai.format_as_xml import format_as_xml\n",
    "\n",
    "@agent.system_prompt\n",
    "async def system_prompt() -> str:\n",
    "    return f\"\"\"\\\n",
    "Given the following MSSQL table of records, your job is to\n",
    "write a SQL query that suits the user's request.  Please make sure not to\n",
    "include Limit in the query.\n",
    "\n",
    "Database schema:\n",
    "\n",
    "{DB_SCHEMA}\n",
    "\n",
    "today's date = {date.today()}\n",
    "\n",
    "{format_as_xml(SQL_EXAMPLES)}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the methods to connect to a SQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aioodbc\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "# Database connection details\n",
    "server = 'sql.denallix.com'  # e.g., 'localhost' or '192.168.1.1'\n",
    "database = 'K2Applications'\n",
    "username = 'WorkflowReporting'\n",
    "password = ''\n",
    "\n",
    "dsn = None  # Optional: Use DSN if configured\n",
    "\n",
    "def get_connection_string():\n",
    "    return (\n",
    "        f'DRIVER={{SQL Server}};'\n",
    "        f'SERVER={server};'\n",
    "        f'DATABASE={database};'\n",
    "        f'UID={username};'\n",
    "        f'PWD={password}'\n",
    "    )\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def connect_to_db():\n",
    "    conn = await aioodbc.connect(dsn=get_connection_string())\n",
    "    try:\n",
    "        yield conn\n",
    "    finally:\n",
    "        await conn.close()\n",
    "\n",
    "async def execute_query(query, params=()):\n",
    "    async with connect_to_db() as conn:\n",
    "        try:\n",
    "            async with conn.cursor() as cursor:\n",
    "                await cursor.execute(query, params)\n",
    "                results = await cursor.fetchall()\n",
    "                for row in results:\n",
    "                    print(row)\n",
    "                return results\n",
    "        except Exception as e:\n",
    "            print(\"Error executing query:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask a question, send it to the LLM, along with the system prompts. \n",
    "See what SQL is generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Generating SQL query...\n",
      "Generated SQL Query:\n",
      "SELECT Departments, COUNT(*) AS NumberOfWorkflows, \n",
      "       SUM(CASE WHEN Status = 'Completed' THEN 1 ELSE 0 END) AS CompletedWorkflows,\n",
      "       SUM(CASE WHEN Status = 'Failed' THEN 1 ELSE 0 END) AS FailedWorkflows,\n",
      "       SUM(CASE WHEN Status = 'In Progress' THEN 1 ELSE 0 END) AS InProgressWorkflows,\n",
      "       SUM(CASE WHEN Status = 'Cancelled' THEN 1 ELSE 0 END) AS CancelledWorkflows,\n",
      "       SUM(CASE WHEN Status = 'Started' THEN 1 ELSE 0 END) AS StartedWorkflows\n",
      "FROM WorkflowReporting.WorkflowInstances\n",
      "GROUP BY Departments;\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Generate SQL from user input\n",
    "\n",
    "prompt = \"Can you give a summary of workflows submitted by department?\"\n",
    "print(\"Step 1: Generating SQL query...\")\n",
    "result = await agent.run(prompt)\n",
    "\n",
    "if isinstance(result.data, InvalidRequest):\n",
    "    print(\"Invalid request:\", result.data.error_message)\n",
    "    \n",
    "    \n",
    "sql_query = result.data.sql_query\n",
    "#print(\"Generated SQL Query:\", sql_query)\n",
    "print(\"Generated SQL Query:\")\n",
    "print(f\"{sql_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go and execute that SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Executing the SQL query\n",
      "Query Results:\n",
      "('Finance', 6, 1, 1, 0, 3, 1)\n",
      "('HR', 11, 5, 0, 3, 1, 2)\n",
      "('IT', 7, 1, 1, 1, 1, 3)\n",
      "('Marketing', 18, 1, 1, 4, 10, 2)\n",
      "('Operations', 8, 1, 0, 0, 5, 2)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Execute the SQL query\n",
    "print(\"Step 2: Executing the SQL query\")\n",
    "async with connect_to_db() as conn:\n",
    "    try:\n",
    "        async with conn.cursor() as cursor:\n",
    "            await cursor.execute(sql_query)\n",
    "            results = await cursor.fetchall()\n",
    "            print(\"Query Results:\")\n",
    "            for row in results:\n",
    "                print(f\"{row}\")\n",
    "            #return results\n",
    "    except Exception as e:\n",
    "        print(\"Error executing query:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, send everything back to the LLM, so it can give a well formatted answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Requesting summary from the LLM...\n",
      "Summary of Results:\n",
      "Certainly! Let's break down the summary of workflows submitted by each department:\n",
      "\n",
      "1. **Finance**:\n",
      "   - **Total Workflows**: 6\n",
      "   - **Pattern**: The Finance department has a balanced distribution across the workflow stages, with most workflows in the initial stage and one workflow stuck in the later stages. This suggests a relatively smooth process.\n",
      "\n",
      "2. **HR**:\n",
      "   - **Total Workflows**: 11\n",
      "   - **Notable Insights**: HR has the highest number of submitted workflows. Most of their workflows are in the early stages, indicating a potential bottleneck or a need for resource reallocation to progress them further.\n",
      "\n",
      "3. **IT**:\n",
      "   - **Total Workflows**: 7\n",
      "   - **Interesting Point**: IT has a steady advancement of workflows, with a slightly higher number in the final stages, possibly reflecting efficient processing or complexities being resolved effectively.\n",
      "\n",
      "4. **Marketing**:\n",
      "   - **Total Workflows**: 18\n",
      "   - **Key Observations**: Marketing leads with the most workflows completed, highlighting their efficiency in moving workflows to completion. A significant portion is also in the advanced stages, showcasing effective workflow management.\n",
      "\n",
      "5. **Operations**:\n",
      "   - **Total Workflows**: 8\n",
      "   - **Highlight**: Operations has several workflows in the middle stages, suggesting momentum but possible challenges in advancing to completion. This could imply reconsideration of bottlenecks or process adjustments.\n",
      "\n",
      "**Overall Insights**:\n",
      "- The **Marketing** department stands out with the highest number of workflows processed to completion, which reflects robust process management.\n",
      "- The **HR** department's high volume of workflows in the initial stages may need attention to ensure a smoother transition through subsequent stages.\n",
      "- The balanced distribution in **IT** and **Finance** suggests stability, while **Operations** may benefit from analyzing mid-stage workflows to enhance flow.\n",
      "\n",
      "This overview not only highlights each department's performance but also provides an opportunity to identify areas for improvement and strategic resource allocation.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Pass results back to LLM for summarization\n",
    "print(\"Step 3: Requesting summary from the LLM...\")\n",
    "summary_prompt = f\"\"\"\n",
    "The following quesion was asked:\n",
    "{prompt}\n",
    " \n",
    "\n",
    "Here are the results of that query:\n",
    "{results}\n",
    "Please answer the question in a user-friendly and informative way. Highlight any patterns, notable values, or interesting insights. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "summary_agent = Agent(\"openai:gpt-4o\", result_type=str)\n",
    "summary = await summary_agent.run(summary_prompt)\n",
    "\n",
    "\n",
    "print(\"Summary of Results:\")\n",
    "print(f\"{summary.data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
